import numpy as np
import os # Import os for checking file existence

# ---------- PARAMETERS ----------
# These parameters MUST match the ones used during the training/export phase
# where train_88.mem, test_88.mem, and the weight/bias files were generated.
# Assuming:
# - Inputs (pixels) in test_88.mem are Q8.8 (16 bits total, 8 integer bits including sign)
# - Weights are Q8.8 (16 bits total, 8 integer bits including sign)
# - Biases are Q16.16 (32 bits total, 16 integer bits including sign)
dims = [64, 50, 30, 10] # Corrected hidden_dim to 50, assuming consistency with training script

# Fixed-point format for inputs and weights (Q8.8)
n_in_w, x_in_w = 32, 16

# Fixed-point format for weights (Q8.8)
n_w, x_w = 32, 16

# Fixed-point format for biases (Q16.16 as per your request)
n_b, x_b = 64, 32

# ---------- Fixed-Point Helpers ----------
def fixed_bin_to_float(bin_str, n, x):
    """
    Converts a 2's complement binary string to a floating-point value
    based on the specified total bits (n) and integer bits (x).
    """
    if not bin_str:
        return 0.0 # Handle empty string case gracefully

    val = int(bin_str, 2) # Convert binary string to integer
    
    # Check if the number is negative (MSB is 1)
    if bin_str[0] == '1':
        val -= (1 << n) # Subtract 2^n to get the correct negative value
    
    # Divide by 2^(n-x) to get the floating-point value
    # (n-x) is the number of fractional bits
    return val / (2 ** (n - x))

def relu(x):
    """ReLU activation function."""
    return np.maximum(0, x)

def softmax(x):
    """Softmax activation function for the output layer."""
    # Subtracting np.max(x) for numerical stability to prevent overflow
    # when computing exponentials of large numbers.
    x = x - np.max(x)
    exp_x = np.exp(x)
    return exp_x / np.sum(exp_x)

# ---------- LOAD WEIGHTS & BIASES ----------
weights = []
biases = []

print("Loading weights and biases...")
for layer_idx in range(1, len(dims)):
    in_dim = dims[layer_idx - 1]
    out_dim = dims[layer_idx]

    weights_file = f"layer{layer_idx}_weights.mem"
    biases_file = f"layer{layer_idx}_biases.mem"

    if not os.path.exists(weights_file) or not os.path.exists(biases_file):
        print(f"Error: Missing weight/bias files for layer {layer_idx}.")
        print(f"Please ensure '{weights_file}' and '{biases_file}' exist and were generated by the training script with matching fixed-point formats.")
        exit()

    # Load weights (Q8.8, so each weight is n_w bits long)
    with open(weights_file, 'r') as wf:
        w_layer = []
        for line in wf:
            line = line.strip()
            if not line: continue # Skip empty lines
            # Each line represents a row (for one output neuron)
            # Each row has in_dim weights, each n_w bits long
            row = [fixed_bin_to_float(line[j*n_w:(j+1)*n_w], n_w, x_w) for j in range(in_dim)]
            w_layer.append(np.array(row)) # Convert row to numpy array
        weights.append(np.array(w_layer))  # shape: (out_dim, in_dim)
    print(f"Loaded {weights_file} (expected {n_w}-bit values)")

    # Load biases (Q16.16, so each bias is n_b bits long)
    with open(biases_file, 'r') as bf:
        b_layer = []
        for line in bf:
            line = line.strip()
            if not line: continue # Skip empty lines
            b_layer.append(fixed_bin_to_float(line, n_b, x_b))
        biases.append(np.array(b_layer))  # shape: (out_dim,)
    print(f"Loaded {biases_file} (expected {n_b}-bit values)")

# ---------- LOAD TEST DATA ----------
X_test = []
y_test = []

test_data_file = "test_88.mem"
if not os.path.exists(test_data_file):
    print(f"Error: {test_data_file} not found. Please ensure it's in the same directory.")
    exit()

print(f"\nLoading test data from {test_data_file}...")
with open(test_data_file, "r") as f:
    for line in f:
        line = line.strip()
        if not line: continue # Skip empty lines
        
        # Pixel data is 64 pixels * n_in_w bits
        # Label is 4 bits
        pixel_bits = line[:-4]
        label_bits = line[-4:]

        # Each pixel is n_in_w bits long (16 bits)
        image = [fixed_bin_to_float(pixel_bits[i*n_in_w:(i+1)*n_in_w], n_in_w, x_in_w) for i in range(dims[0])]
        label = int(label_bits, 2)

        X_test.append(image)
        y_test.append(label)

X_test = np.array(X_test, dtype=np.float32)
y_test = np.array(y_test, dtype=np.int32)
print(f"Loaded {len(X_test)} test samples.")

# ---------- INFERENCE ----------
print("\nStarting inference...")
correct = 0
total = len(X_test)

for idx in range(total):
    x = X_test[idx] # Get current input image

    # Forward pass through the network layers
    for l in range(len(weights)):
        # Matrix multiplication: current_input_vector . weights_matrix.T
        # As loaded, weights[l] is (out_dim, in_dim) and x is (in_dim,)
        x = np.dot(weights[l], x) + biases[l]
        
        # Apply activation function
        if l < len(weights) - 1: # Apply ReLU for hidden layers
            x = relu(x)
        else: # Apply Softmax for the output layer
            x = softmax(x)

    # Get the predicted class (index with highest probability)
    pred = np.argmax(x)
    
    # Check if prediction matches the true label
    if pred == y_test[idx]:
        correct += 1

accuracy = correct / total * 100
print(f"\nTest Accuracy: {accuracy:.2f}%")
